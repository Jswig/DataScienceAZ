---
title: "Predicting Startup Profitability"
author: "Anders Poirel"
output: github_document
---

*Another exercise done using dataset from the Data Science A-Z course.*
```{r, echo=FALSE}
library(fastDummies)
```

Load the data into R:
```{r}
startup_data <- read.csv('P12-50-Startups.csv')
```

Create dummy variables to subsitute for states in which the startups operate, and remove the original 'State' column:
```{r}
startup_data <- dummy_cols(startup_data, select_columns = "State", remove_first_dummy = TRUE)
startup_data <- startup_data[,-4]
```
Above, I omit the first dummy variable with ``remove_first_dummy = TRUE`` to avoid multicolinearity issues.

To avoid overfitting the model, we use backward elimination to progressively eliminate poor predictors. I use an (arbitrary) significance level of p = 0.1 as the criterion for elemination:

```{r}
fit <- lm(startup_data$Profit ~ startup_data$R.D.Spend + startup_data$Administration + startup_data$Marketing.Spend + startup_data$State_California, data = startup_data)
summary(fit)
```
"Administration"" has the highest p-value (0.651 > 0.1) so I eliminate for the next step of the fitting process:
```{r}
fit <- lm(startup_data$Profit ~ startup_data$R.D.Spend + startup_data$Marketing.Spend + startup_data$State_California, data = startup_data)
summary(fit)
```
Same for startup location ("State_California"):
```{r}

fit <- fit <- lm(startup_data$Profit ~ startup_data$R.D.Spend + startup_data$Marketing.Spend , data = startup_data)
summary(fit)
```
None of the remaining prdictors have a p-value above 0.1 so the process of backwards elimination is complete.

